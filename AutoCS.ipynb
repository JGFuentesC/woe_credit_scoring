{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "from CreditScoringToolkit import frequency_table\n",
    "from CreditScoringToolkit import DiscreteNormalizer\n",
    "from CreditScoringToolkit import WoeEncoder\n",
    "from CreditScoringToolkit import WoeContinuousFeatureSelector\n",
    "from CreditScoringToolkit import WoeDiscreteFeatureSelector\n",
    "from CreditScoringToolkit import CreditScoring\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from scipy.stats.mstats import winsorize\n",
    "from varclushi import VarClusHi\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import logging\n",
    "from typing import List, Tuple,Optional,Dict\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"sklearn.preprocessing._discretization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 23)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('example_data/train.csv')\n",
    "valid = pd.read_csv('example_data/valid.csv')   \n",
    "data = pd.concat([train,valid],ignore_index=True).sample(frac=1)\n",
    "varc = [v for v in data.columns if v[:2]=='C_']\n",
    "vard = [v for v in data.columns if v[:2]=='D_']\n",
    "target = 'TARGET'\n",
    "for v in varc:\n",
    "    data[v] = pd.to_numeric(data[v],errors='coerce')\n",
    "for v in vard:\n",
    "    data[v] = data[v].fillna('MISSING').astype(str)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logger = logging.getLogger(\"CreditScoringToolkit\")\n",
    "if logger.hasHandlers():\n",
    "    logger.handlers.clear()\n",
    "logger.setLevel(logging.INFO)\n",
    "handler = logging.StreamHandler()\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "logger.addHandler(handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoCreditScoring:\n",
    "    continuous_features: List[str]\n",
    "    discrete_features: List[str]\n",
    "    target: str\n",
    "    data: pd.DataFrame\n",
    "    train: pd.DataFrame\n",
    "    valid: pd.DataFrame\n",
    "    apply_multicolinearity: bool = False \n",
    "    iv_feature_threshold: float = 0.05\n",
    "    treat_outliers: bool = False\n",
    "    outlier_threshold: float = 0.01\n",
    "    min_score = 400\n",
    "    max_score = 900\n",
    "    max_discretization_bins = 5\n",
    "    discrete_normalization_threshold = 0.05\n",
    "    discrete_normalization_default_category = 'OTHER'\n",
    "    transformation: Optional[str] = None\n",
    "    model: Optional[LogisticRegression] = None\n",
    "    max_iter: int = 5 \n",
    "    train_size: float = 0.7\n",
    "    target_proportion_tolerance: float = 0.01\n",
    "    max_discretization_bins:int=6\n",
    "    strictly_monotonic:bool=True\n",
    "    discretization_method:str = 'quantile'\n",
    "    n_threads:int = 1 \n",
    "    overfitting_tolerance:float = 0.01\n",
    "\n",
    "    def __init__(self, data: pd.DataFrame, target: str, continuous_features: List[str]=None, discrete_features: List[str]=None):\n",
    "        self.data = data\n",
    "        self.continuous_features = continuous_features\n",
    "        self.discrete_features = discrete_features\n",
    "        self.target = target\n",
    "\n",
    "    def fit(self,\n",
    "            target_proportion_tolerance:float = None, \n",
    "            treat_outliers:bool = None, \n",
    "            discrete_normalization_threshold:float = None,\n",
    "            discrete_normalization_default_category:str = None,\n",
    "            max_discretization_bins:int = None,\n",
    "            strictly_monotonic:bool = None,\n",
    "            iv_feature_threshold:float = None,\n",
    "            discretization_method:str = None,\n",
    "            n_threads:int = None,\n",
    "            overfitting_tolerance:float = None,\n",
    "            verbose:bool=False):\n",
    "        # Verbosity control\n",
    "        if verbose:\n",
    "            logger.setLevel(logging.INFO)\n",
    "        else:\n",
    "            logger.setLevel(logging.WARNING)\n",
    "        # Check if continuous_features is provided\n",
    "        if self.continuous_features is None:\n",
    "            self.continuous_features = []\n",
    "            logger.warning(\"No continuous features provided\")\n",
    "        # Check if discrete_features is provided\n",
    "        if self.discrete_features is None:\n",
    "            self.discrete_features = []\n",
    "            logger.warning(\"No discrete features provided\")\n",
    "        if len(self.continuous_features)==0 and len(self.discrete_features)==0:\n",
    "            logger.error(\"No features provided\")\n",
    "            raise RuntimeError(\"No features provided\")\n",
    "        \n",
    "        # Check if target_proportion_tolerance is provided\n",
    "        if target_proportion_tolerance:\n",
    "            self.target_proportion_tolerance = target_proportion_tolerance\n",
    "        # Partition data\n",
    "        self.__partition_data()\n",
    "        \n",
    "        #Check if treat_outliers is provided\n",
    "        if len(self.continuous_features)>0 and treat_outliers:\n",
    "            self.treat_outliers = treat_outliers\n",
    "            self.__outlier_treatment()\n",
    "            \n",
    "        # Check if discrete_normalization_threshold is provided\n",
    "        if discrete_normalization_threshold:\n",
    "            self.discrete_normalization_threshold = discrete_normalization_threshold\n",
    "        # Check if discrete_normalization_default_category is provided\n",
    "        if discrete_normalization_default_category:\n",
    "            self.discrete_normalization_default_category = discrete_normalization_default_category\n",
    "        if len(self.discrete_features)==0:\n",
    "            logger.warning(\"No discrete features provided\")\n",
    "        else:\n",
    "            if len(self.discrete_features)>0:\n",
    "                # Normalize discrete features\n",
    "                self.__normalize_discrete()\n",
    "            \n",
    "        #Check feature selection parameters\n",
    "        if max_discretization_bins:\n",
    "            self.max_discretization_bins = max_discretization_bins\n",
    "        if strictly_monotonic:\n",
    "            self.strictly_monotonic = strictly_monotonic\n",
    "        if iv_feature_threshold:\n",
    "            self.iv_feature_threshold = iv_feature_threshold\n",
    "        if discretization_method:\n",
    "            self.discretization_method = discretization_method\n",
    "        if n_threads:\n",
    "            self.n_threads = n_threads\n",
    "\n",
    "        # Feature selection\n",
    "        self.__feature_selection()\n",
    "\n",
    "        # Woe transformation\n",
    "        self.__woe_transformation()\n",
    "\n",
    "        # Check if overfitting_tolerance is provided\n",
    "        if overfitting_tolerance:\n",
    "            self.overfitting_tolerance = overfitting_tolerance\n",
    "        # Train model\n",
    "        self.__train_model()\n",
    "        \n",
    "    def __partition_data(self):\n",
    "        logger.info(\"Partitioning data...\")\n",
    "        self.train, self.valid = train_test_split(self.data, train_size=self.train_size)\n",
    "        self.train.reset_index(drop=True, inplace=True)\n",
    "        self.valid.reset_index(drop=True, inplace=True)\n",
    "        # Check if target proportions are compatible between train and valid\n",
    "        logger.info(\"Checking partition proportions...\")\n",
    "        iter = 1\n",
    "        while(np.abs(self.train[self.target].mean()-self.valid[self.target].mean())>self.target_proportion_tolerance):\n",
    "            logger.info(f\"Partitioning data...Iteration {iter}\")\n",
    "            logger.info(f\"Train target proportion: {self.train[target].mean()}\")\n",
    "            logger.info(f\"Valid target proportion: {self.valid[target].mean()}\")\n",
    "            self.train, self.valid = train_test_split(self.data, train_size=self.train_size)\n",
    "            self.train.reset_index(drop=True, inplace=True)\n",
    "            self.valid.reset_index(drop=True, inplace=True)\n",
    "            iter+=1\n",
    "            if iter>self.max_iter:\n",
    "                logger.error(\"Could not find a compatible partition\")\n",
    "                raise RuntimeError(\"Could not find a compatible partition\")\n",
    "            \n",
    "        if iter>1:\n",
    "            logger.info(f\"Partitioning data...Done after {iter} iterations\")\n",
    "        logger.info(f\"Train shape: {self.train.shape}\", )\n",
    "        logger.info(f\"Test shape: {self.valid.shape}\")\n",
    "        logger.info(f\"Train target proportion: {self.train[target].mean()}\")\n",
    "        logger.info(f\"Valid target proportion: {self.valid[target].mean()}\")\n",
    "\n",
    "    def __outlier_treatment(self):\n",
    "        logger.info(\"Outlier treatment...\")\n",
    "        before = self.train[self.continuous_features].mean()\n",
    "        for f in self.continuous_features:\n",
    "            self.train[f] = winsorize(self.train[f], limits=[self.outlier_threshold, self.outlier_threshold])\n",
    "        after = self.train[self.continuous_features].mean()\n",
    "        report = pd.DataFrame({'Before':before,'After':after})\n",
    "        logger.info(\"Mean statistics before and after outlier treatment\")\n",
    "        logger.info(f'\\n\\n{report}\\n')\n",
    "        logger.info(\"Outlier treatment...Done\")        \n",
    "\n",
    "    def __normalize_discrete(self):\n",
    "        logger.info(\"Discrete normalization...\")\n",
    "        logger.info(f\"Discrete features: {self.discrete_features}\")\n",
    "        dn = DiscreteNormalizer(normalization_threshold=self.discrete_normalization_threshold, \n",
    "                                default_category=self.discrete_normalization_default_category)\n",
    "        dn.fit(self.train[self.discrete_features])\n",
    "        self.train_discrete_normalized = dn.transform(self.train[self.discrete_features])\n",
    "        logger.info(\"Checking if normalization produced unary columns\")\n",
    "        self.unary_columns = [c for c in self.train_discrete_normalized.columns if self.train_discrete_normalized[c].nunique()==1]\n",
    "        if len(self.unary_columns)>0:\n",
    "            logger.warning(f\"Normalization produced unary columns: {self.unary_columns}\")\n",
    "            logger.warning(f\"Removing unary columns from discrete features\")\n",
    "            self.discrete_features = [f for f in self.discrete_features if f not in self.unary_columns]\n",
    "            logger.warning(f\"Discrete features after unary columns removal: {self.discrete_features}\")\n",
    "        else:\n",
    "            logger.info(\"No unary columns produced by normalization\")\n",
    "        if len(self.discrete_features)==0:\n",
    "            logger.warning(\"No discrete features left after normalization\")\n",
    "        else:\n",
    "            dn.fit(self.train[self.discrete_features])\n",
    "            self.train_discrete_normalized = dn.transform(self.train[self.discrete_features])\n",
    "        self.discrete_normalizer = dn \n",
    "        logger.info(\"Discrete normalization...Done\")\n",
    "\n",
    "    def __feature_selection(self):\n",
    "        try:\n",
    "            logger.info(\"Feature selection...\")\n",
    "            if len(self.continuous_features)>0:\n",
    "                logger.info(\"Continuous features selection...\")\n",
    "                woe_continuous_selector = WoeContinuousFeatureSelector()\n",
    "                woe_continuous_selector.fit(self.train[self.continuous_features], self.train[self.target],\n",
    "                    max_bins=self.max_discretization_bins,\n",
    "                    strictly_monotonic=self.strictly_monotonic,\n",
    "                    iv_threshold=self.iv_feature_threshold,\n",
    "                    method=self.discretization_method,\n",
    "                    n_threads=self.n_threads)\n",
    "                self.iv_report_continuous = pd.DataFrame(woe_continuous_selector.selected_features)\n",
    "                self.full_iv_report_continuous = woe_continuous_selector.iv_report.copy()\n",
    "                self.continuous_candidate = woe_continuous_selector.transform(self.train[self.continuous_features])\n",
    "                logger.info(f'\\n\\n{self.iv_report_continuous}\\n\\n')\n",
    "                self.woe_continuous_selector = woe_continuous_selector\n",
    "                logger.info(f\"Continuous features selection...Done\")\n",
    "            if len(self.discrete_features)>0:\n",
    "                logger.info(\"Discrete features selection...\")\n",
    "                woe_discrete_selector = WoeDiscreteFeatureSelector()\n",
    "                woe_discrete_selector.fit(self.train_discrete_normalized, self.train[self.target],self.iv_feature_threshold)\n",
    "                self.iv_report_discrete = pd.Series(woe_discrete_selector.selected_features).to_frame('iv').reset_index().rename(columns={'index':'feature'}).sort_values('iv',ascending=False)\n",
    "                self.full_iv_report_discrete = woe_discrete_selector.iv_report.copy()\n",
    "                self.discrete_candidate = woe_discrete_selector.transform(self.train_discrete_normalized)\n",
    "                logger.info(f'\\n\\n{self.iv_report_discrete}\\n\\n')\n",
    "                self.woe_discrete_selector = woe_discrete_selector\n",
    "                logger.info(\"Discrete features selection...Done\")    \n",
    "            \n",
    "            if len(self.continuous_features)>0 and len(self.discrete_features)>0:\n",
    "                logger.info(\"Merging continuous and discrete features...\")\n",
    "                self.train_candidate = pd.concat([self.continuous_candidate, self.discrete_candidate], axis=1)\n",
    "                logger.info(\"Merging continuous and discrete features...Done\")\n",
    "            elif len(self.continuous_features)>0:\n",
    "                self.train_candidate = self.continuous_candidate\n",
    "            elif len(self.discrete_features)>0:\n",
    "                self.train_candidate = self.discrete_candidate\n",
    "            self.candidate_features = list(self.train_candidate.columns)\n",
    "            if len(self.candidate_features)==0:\n",
    "                logger.error(\"No features selected\")\n",
    "                raise RuntimeError(\"No features selected\")\n",
    "            logger.info(f\"Selected features ({len(self.candidate_features)}): {self.candidate_features}\")\n",
    "            logger.info(\"Feature selection...Done\")\n",
    "        except Exception as err:\n",
    "            logger.error(f\"Error in feature selection: {err}\")\n",
    "            raise err\n",
    "\n",
    "    def __woe_transformation(self):\n",
    "        self.woe_encoder = WoeEncoder()\n",
    "        self.woe_encoder.fit(self.train_candidate, self.train[self.target])\n",
    "        self.train_woe = self.woe_encoder.transform(self.train_candidate)\n",
    "        if self.train_woe.isna().max().max():\n",
    "            logger.error(\"NAs found in transformed data\")\n",
    "            raise RuntimeError(\"NAs found in transformed data, Maybe tiny missing in continuous?\")\n",
    "    \n",
    "    def __apply_pipeline(self,data:pd.DataFrame)->pd.DataFrame:\n",
    "        try:\n",
    "            if len(self.continuous_features)>0:\n",
    "                if self.treat_outliers:\n",
    "                    for f in self.continuous_features:\n",
    "                        data[f] = winsorize(data[f], limits=[self.outlier_threshold, self.outlier_threshold])\n",
    "                data_continuous_candidate = self.woe_continuous_selector.transform(data[self.continuous_features])\n",
    "            if len(self.discrete_features)>0:\n",
    "                data_discrete_normalized = self.discrete_normalizer.transform(data[self.discrete_features])\n",
    "                data_discrete_candidate = self.woe_discrete_selector.transform(data_discrete_normalized)\n",
    "            if len(self.continuous_features)>0 and len(self.discrete_features)==0:\n",
    "                data_candidate = data_continuous_candidate.copy()\n",
    "            if len(self.continuous_features)==0 and len(self.discrete_features)>0:\n",
    "                data_candidate = data_discrete_candidate.copy()\n",
    "            if len(self.continuous_features)>0 and len(self.discrete_features)>0:\n",
    "                data_candidate = pd.concat([data_continuous_candidate, data_discrete_candidate], axis=1)\n",
    "            data_woe = self.woe_encoder.transform(data_candidate)\n",
    "            if data_woe.isna().max().max():\n",
    "                logger.error(\"NAs found in transformed data\")\n",
    "                raise RuntimeError(\"NAs found in transformed data, Maybe tiny missing in continuous?\")\n",
    "            return data_woe\n",
    "        except Exception as err:\n",
    "            logger.error(f\"Error applying pipeline: {err}\")\n",
    "            raise err\n",
    "    \n",
    "    def __train_model(self):\n",
    "        logger.info(\"Training model...\")\n",
    "        lr = LogisticRegression()\n",
    "        lr.fit(self.train_woe,self.train[self.target])\n",
    "        self.model = lr\n",
    "        self.valid_woe = self.__apply_pipeline(self.valid)\n",
    "        self.auc_train = roc_auc_score(y_score=lr.predict_proba(self.train_woe)[:,1],y_true=self.train[self.target])\n",
    "        self.auc_valid = roc_auc_score(y_score=lr.predict_proba(self.valid_woe)[:,1],y_true=self.valid[self.target])\n",
    "        logger.info(f\"AUC for training: {self.auc_train}\")\n",
    "        logger.info(f\"AUC for validation:{self.auc_valid}\")\n",
    "        self.betas = lr.coef_[0]\n",
    "        self.alpha = lr.intercept_[0]\n",
    "        if any([np.abs(b)<0.0001 for b in self.betas]):\n",
    "            logger.warning(\"Some betas are close to zero, consider removing features\")\n",
    "            logger.warning(f\"Betas: {dict(zip(self.candidate_features,self.betas))}\")\n",
    "            logger.warning(f\"Suspicious features: {[f for f,b in zip(self.candidate_features,self.betas) if np.abs(b)<0.0001]}\")\n",
    "        if abs(self.auc_train-self.auc_valid)>self.overfitting_tolerance:\n",
    "            logger.warning(f\"Overfitting detected, review your hyperparameters. train_auc{self.auc_train}, valid_auc{self.auc_valid}\")\n",
    "        self.logistic_model = lr\n",
    "        logger.info(\"Training model...Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/josegustavofuentescabrera/entornos/woe/lib/python3.13/site-packages/sklearn/preprocessing/_discretization.py:306: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "/Users/josegustavofuentescabrera/entornos/woe/lib/python3.13/site-packages/sklearn/preprocessing/_discretization.py:306: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "/Users/josegustavofuentescabrera/entornos/woe/lib/python3.13/site-packages/sklearn/preprocessing/_discretization.py:306: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "/Users/josegustavofuentescabrera/entornos/woe/lib/python3.13/site-packages/sklearn/preprocessing/_discretization.py:306: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "/Users/josegustavofuentescabrera/code/woe-credit-scoring/woe_credit_scoring/CreditScoringToolkit.py:544: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  aux[feature] = aux[feature].replace(woe_map)\n",
      "/Users/josegustavofuentescabrera/code/woe-credit-scoring/woe_credit_scoring/CreditScoringToolkit.py:544: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  aux[feature] = aux[feature].replace(woe_map)\n",
      "2025-02-28 18:18:02,663 - CreditScoringToolkit - WARNING - Overfitting detected, review your hyperparameters. train_auc0.6920364102699701, valid_auc0.6385967665986501\n"
     ]
    }
   ],
   "source": [
    "acs = AutoCreditScoring(data, target, continuous_features=varc, discrete_features=vard)\n",
    "acs.fit(verbose=False, \n",
    "        target_proportion_tolerance=0.01,\n",
    "        treat_outliers=True,\n",
    "        discrete_normalization_threshold=0.1,\n",
    "        discrete_normalization_default_category='XYZW',\n",
    "        max_discretization_bins=5,\n",
    "        strictly_monotonic=True,\n",
    "        iv_feature_threshold=0.05,\n",
    "        discretization_method='quantile',\n",
    "        n_threads=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WOE Credit Scoring",
   "language": "python",
   "name": "woe"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
